{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from clap import Clap\n",
    "from clap.evaluate import eval_retrieval\n",
    "from clap.datasets import ClapDataset\n",
    "from clap.utils import get_target_device, load_clap_config"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Compute retrieval performance of CLAP on AudioCaps and Clotho",
   "id": "6e3ddec691e6da38"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Load config for audio processing and get target device\n",
    "audio_encoder = \"htsat-tiny\"\n",
    "text_encoder = \"gpt2\"\n",
    "cfg_version = 1\n",
    "ckpt_version = 3\n",
    "config = load_clap_config(audio_encoder=audio_encoder, text_encoder=text_encoder, version=cfg_version)\n",
    "device = get_target_device()"
   ],
   "id": "36092738f820f024",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Initialize evaluation datasets and dataloaders\n",
    "audio_caps_eval_dataset = ClapDataset(config=config, datasets=[\"AudioCaps\"], kinds=[\"test\"])\n",
    "audio_caps_loader = DataLoader(audio_caps_eval_dataset, batch_size=64, shuffle=False)\n",
    "clotho_eval_dataset = ClapDataset(config=config, datasets=[\"Clotho\"], kinds=[\"test\"])\n",
    "clotho_loader = DataLoader(clotho_eval_dataset, batch_size=64, shuffle=False)"
   ],
   "id": "15066a7ac7ca9cfa",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Load trained model\n",
    "clap = Clap.from_ckpt(audio_encoder=audio_encoder, text_encoder=text_encoder, cfg_version=cfg_version, ckpt_version=ckpt_version).to(device)"
   ],
   "id": "bdcc2033237baec",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Get metrics\n",
    "audio_caps_metrics = eval_retrieval(model=clap, test_loader=audio_caps_loader)\n",
    "clotho_metrics = eval_retrieval(model=clap, test_loader=clotho_loader)"
   ],
   "id": "6690071f3877c10c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(\"Audio Caps:\\n\")\n",
    "for name, score in audio_caps_metrics.items():\n",
    "    print(f\"{name:14}: {score:.4f}\")"
   ],
   "id": "a77d73626d98e5b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(\"Clotho:\\n\")\n",
    "for name, score in clotho_metrics.items():\n",
    "    print(f\"{name:14}: {score:.4f}\")"
   ],
   "id": "b5e58bf911d7f847",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
