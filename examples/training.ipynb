{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-28T20:24:50.255145Z",
     "start_time": "2024-07-28T20:24:43.370910Z"
    }
   },
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import wandb as wb\n",
    "from clap import Clap, ClapDataset, ClapTrainer, SymmetricCrossEntropyLoss, get_target_device, load_config\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import optim"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-28T20:24:50.423288Z",
     "start_time": "2024-07-28T20:24:50.257149Z"
    }
   },
   "cell_type": "code",
   "source": [
    "config_path = \"clap/configs/clap_htsat-tiny_gpt2.yml\"\n",
    "config = load_config(config_path)\n",
    "device = get_target_device()"
   ],
   "id": "200215a64d7b9d33",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-28T20:29:15.465912Z",
     "start_time": "2024-07-28T20:29:03.681280Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# change these accordingly\n",
    "seed = ClapTrainer.set_random_seed(None)\n",
    "train_dataset = ClapDataset(config=config_path, kind=\"train\")\n",
    "val_dataset = ClapDataset(config=config_path, kind=\"val\")\n",
    "test_dataset = ClapDataset(config=config_path, kind=\"test\")"
   ],
   "id": "e768d189b841f464",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random seed set as 2831676980\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "id": "b8f0357b0121987b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-28T20:25:06.885309Z",
     "start_time": "2024-07-28T20:25:04.299413Z"
    }
   },
   "source": [
    "wb.login()"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[34m\u001B[1mwandb\u001B[0m: Currently logged in as: \u001B[33mleonakkad\u001B[0m. Use \u001B[1m`wandb login --relogin`\u001B[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "id": "7f1c5b721f62cd81",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-28T20:25:10.278610Z",
     "start_time": "2024-07-28T20:25:06.887319Z"
    }
   },
   "source": [
    "wb.init(\n",
    "    # set the wandb project where this run will be logged \n",
    "    project='Custom-CLAP',\n",
    "    name=\"Test run with hdf5 file\",\n",
    "    # track hyperparameters\n",
    "    config=config\n",
    ")\n",
    "config = wb.config"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "wandb version 0.17.5 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Tracking run with wandb version 0.17.1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\leon\\Documents\\ML_Projects\\Custom-CLAP\\wandb\\run-20240728_222507-d3brpq5k</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/leonakkad/Custom-CLAP/runs/d3brpq5k' target=\"_blank\">Test run with hdf5 file</a></strong> to <a href='https://wandb.ai/leonakkad/Custom-CLAP' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View project at <a href='https://wandb.ai/leonakkad/Custom-CLAP' target=\"_blank\">https://wandb.ai/leonakkad/Custom-CLAP</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run at <a href='https://wandb.ai/leonakkad/Custom-CLAP/runs/d3brpq5k' target=\"_blank\">https://wandb.ai/leonakkad/Custom-CLAP/runs/d3brpq5k</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "id": "9adf38907f6d43cd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-28T20:29:15.593652Z",
     "start_time": "2024-07-28T20:29:15.467416Z"
    }
   },
   "source": [
    "# define data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=config[\"batch_size\"], shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=config[\"batch_size\"])\n",
    "test_loader = DataLoader(test_dataset, batch_size=config[\"batch_size\"])"
   ],
   "outputs": [],
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "id": "bae277a6c350b548",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-28T20:29:17.262346Z",
     "start_time": "2024-07-28T20:29:15.594655Z"
    }
   },
   "source": [
    "# define model, optimize and loss function\n",
    "model = Clap(config).to(device)\n",
    "print(f\"Number of parameters to train: {sum(p.numel() for p in model.parameters())}\")\n",
    "optimizer = optim.Adam(model.parameters(), lr=config[\"learning_rate\"])\n",
    "loss_fn = SymmetricCrossEntropyLoss()\n",
    "trainer = ClapTrainer(train_loader, val_loader, test_loader, model, optimizer, loss_fn, config[\"epochs\"])"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\leon\\miniconda3\\envs\\custom-clap\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters to train: 167871888\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "id": "73bc9bffbaf50076",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-28T21:17:57.331814Z",
     "start_time": "2024-07-28T20:29:17.263370Z"
    }
   },
   "source": "metrics = trainer.train_and_eval(\"checkpoints/test.ckpt\", None, False)",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting to train Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 1 (lr=array([0.0001])):  31%|███▏      | 314/1002 [48:39<1:46:36,  9.30s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[12], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m metrics \u001B[38;5;241m=\u001B[39m trainer\u001B[38;5;241m.\u001B[39mtrain_and_eval(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcheckpoints/test.ckpt\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m, \u001B[38;5;28;01mFalse\u001B[39;00m)\n",
      "File \u001B[1;32m~\\Documents\\ML_Projects\\Custom-CLAP\\clap\\clap_trainer.py:98\u001B[0m, in \u001B[0;36mClapTrainer.train_and_eval\u001B[1;34m(self, out_path, adapt_lr_factor, early_stopping)\u001B[0m\n\u001B[0;32m     95\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124mStarting to train Model\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m     96\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m epoch \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mepochs):\n\u001B[1;32m---> 98\u001B[0m     train_loss, train_r1_t2a, train_r5_t2a, train_r10_t2a, train_r1_a2t, train_r5_a2t, train_r10_a2t \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtrain_model()\n\u001B[0;32m     99\u001B[0m     val_loss, val_r1_t2a, val_r5_t2a, val_r10_t2a, val_r1_a2t, val_r5_a2t, val_r10_a2t \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39meval_model()\n\u001B[0;32m    101\u001B[0m     \u001B[38;5;66;03m# train_losses.append(train_loss)\u001B[39;00m\n\u001B[0;32m    102\u001B[0m     \u001B[38;5;66;03m# train_accuracies.append(train_acc)\u001B[39;00m\n\u001B[0;32m    103\u001B[0m     \u001B[38;5;66;03m# validation_losses.append(val_loss)\u001B[39;00m\n\u001B[0;32m    104\u001B[0m     \u001B[38;5;66;03m# validation_accuracies.append(val_acc)\u001B[39;00m\n",
      "File \u001B[1;32m~\\Documents\\ML_Projects\\Custom-CLAP\\clap\\clap_trainer.py:319\u001B[0m, in \u001B[0;36mClapTrainer.train_model\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    316\u001B[0m total_r10_a2t\u001B[38;5;241m.\u001B[39mappend(r10_a2t)\n\u001B[0;32m    318\u001B[0m \u001B[38;5;66;03m# Compute the gradients.\u001B[39;00m\n\u001B[1;32m--> 319\u001B[0m loss\u001B[38;5;241m.\u001B[39mbackward()\n\u001B[0;32m    320\u001B[0m \u001B[38;5;66;03m# Perform the update.\u001B[39;00m\n\u001B[0;32m    321\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moptimizer\u001B[38;5;241m.\u001B[39mstep()\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\custom-clap\\Lib\\site-packages\\torch\\_tensor.py:525\u001B[0m, in \u001B[0;36mTensor.backward\u001B[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001B[0m\n\u001B[0;32m    515\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m    516\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[0;32m    517\u001B[0m         Tensor\u001B[38;5;241m.\u001B[39mbackward,\n\u001B[0;32m    518\u001B[0m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    523\u001B[0m         inputs\u001B[38;5;241m=\u001B[39minputs,\n\u001B[0;32m    524\u001B[0m     )\n\u001B[1;32m--> 525\u001B[0m torch\u001B[38;5;241m.\u001B[39mautograd\u001B[38;5;241m.\u001B[39mbackward(\n\u001B[0;32m    526\u001B[0m     \u001B[38;5;28mself\u001B[39m, gradient, retain_graph, create_graph, inputs\u001B[38;5;241m=\u001B[39minputs\n\u001B[0;32m    527\u001B[0m )\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\custom-clap\\Lib\\site-packages\\torch\\autograd\\__init__.py:267\u001B[0m, in \u001B[0;36mbackward\u001B[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[0m\n\u001B[0;32m    262\u001B[0m     retain_graph \u001B[38;5;241m=\u001B[39m create_graph\n\u001B[0;32m    264\u001B[0m \u001B[38;5;66;03m# The reason we repeat the same comment below is that\u001B[39;00m\n\u001B[0;32m    265\u001B[0m \u001B[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001B[39;00m\n\u001B[0;32m    266\u001B[0m \u001B[38;5;66;03m# calls in the traceback and some print out the last line\u001B[39;00m\n\u001B[1;32m--> 267\u001B[0m _engine_run_backward(\n\u001B[0;32m    268\u001B[0m     tensors,\n\u001B[0;32m    269\u001B[0m     grad_tensors_,\n\u001B[0;32m    270\u001B[0m     retain_graph,\n\u001B[0;32m    271\u001B[0m     create_graph,\n\u001B[0;32m    272\u001B[0m     inputs,\n\u001B[0;32m    273\u001B[0m     allow_unreachable\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[0;32m    274\u001B[0m     accumulate_grad\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[0;32m    275\u001B[0m )\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\custom-clap\\Lib\\site-packages\\torch\\autograd\\graph.py:744\u001B[0m, in \u001B[0;36m_engine_run_backward\u001B[1;34m(t_outputs, *args, **kwargs)\u001B[0m\n\u001B[0;32m    742\u001B[0m     unregister_hooks \u001B[38;5;241m=\u001B[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001B[0;32m    743\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 744\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m Variable\u001B[38;5;241m.\u001B[39m_execution_engine\u001B[38;5;241m.\u001B[39mrun_backward(  \u001B[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001B[39;00m\n\u001B[0;32m    745\u001B[0m         t_outputs, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs\n\u001B[0;32m    746\u001B[0m     )  \u001B[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001B[39;00m\n\u001B[0;32m    747\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[0;32m    748\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m attach_logging_hooks:\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "id": "2b40a40bc3d0c553",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-28T21:18:09.584311Z",
     "start_time": "2024-07-28T21:17:58.310474Z"
    }
   },
   "source": [
    "wb.finish()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "23ab26fb887241c3b41c4b7677cea3ea"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>train/a2t/batch recall@1</td><td>▁▁▁▁▂▂▂▁▃▂▂▂▃▃▂▃▅▄▄▅▅▆▃▆▆▆▆▇▆▅▅▆▅▅▇█▄▇█▆</td></tr><tr><td>train/a2t/batch recall@10</td><td>▁▁▁▁▂▂▄▁▃▃▄▄▄▅▄▅▆▆▆▅▅▅▆▇▆▆▆▆▇▆▆▇▇▅▇▇█▇█▇</td></tr><tr><td>train/a2t/batch recall@5</td><td>▁▁▂▁▂▂▃▁▃▃▃▃▃▅▄▄▅▅▅▅▅▅▅▆▆▅▆▅▇▇▆▇▆▅▇▇▇▇█▇</td></tr><tr><td>train/batch loss</td><td>████▇▇▆█▆▇▆▅▆▅▅▅▄▄▄▅▄▄▄▄▃▄▃▃▂▃▃▂▃▅▂▃▂▂▁▂</td></tr><tr><td>train/step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>train/t2a/batch recall@1</td><td>▁▁▁▁▁▁▂▁▂▃▃▄▃▄▃▃▄▃▃▄▄▄▃▃▆▆▇▄▇▆▅▆▅▃▆▆▅▇▇█</td></tr><tr><td>train/t2a/batch recall@10</td><td>▁▂▁▁▂▃▄▁▃▄▄▄▄▅▅▄▅▆▅▅▆▅▆▆▆▆▇▅▇▇▇▇▇▅▇▆▇▇█▇</td></tr><tr><td>train/t2a/batch recall@5</td><td>▁▂▁▂▂▃▃▁▂▃▃▃▃▄▄▃▅▆▅▄▅▅▅▆▆▅▇▅▇▅▇▆▇▅▇▇▇▇█▇</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>train/a2t/batch recall@1</td><td>0.23438</td></tr><tr><td>train/a2t/batch recall@10</td><td>0.79688</td></tr><tr><td>train/a2t/batch recall@5</td><td>0.64062</td></tr><tr><td>train/batch loss</td><td>2.6378</td></tr><tr><td>train/step</td><td>313</td></tr><tr><td>train/t2a/batch recall@1</td><td>0.29688</td></tr><tr><td>train/t2a/batch recall@10</td><td>0.79688</td></tr><tr><td>train/t2a/batch recall@5</td><td>0.625</td></tr></table><br/></div></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">Test run with hdf5 file</strong> at: <a href='https://wandb.ai/leonakkad/Custom-CLAP/runs/d3brpq5k' target=\"_blank\">https://wandb.ai/leonakkad/Custom-CLAP/runs/d3brpq5k</a><br/> View project at: <a href='https://wandb.ai/leonakkad/Custom-CLAP' target=\"_blank\">https://wandb.ai/leonakkad/Custom-CLAP</a><br/>Synced 6 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240728_222507-d3brpq5k\\logs</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 13
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
